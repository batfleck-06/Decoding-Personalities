{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#For data Modeling\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "#For NLP\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "#Miscellaneous\n",
    "\n",
    "# for progress  bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "#regular expressions\n",
    "import re\n",
    "\n",
    "#for .pkl file\n",
    "import joblib\n",
    "\n",
    "#for hyperparameter tuning\n",
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2048798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc33ea8a",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization can work well with text data models as it efficiently explores the hyperparameter space. When tuning hyperparameters for models like text classifiers or sentiment analysis, Bayesian Optimization can help you find the optimal combination of parameters with fewer trials compared to exhaustive methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aaa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb=XGBClassifier(max_depth=5, n_estimators=50, learning_rate=0.1)\n",
    "model_xgb.fit(train_post,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = XGBClassifier(**params)\n",
    "    # Change the scoring metric as needed\n",
    "    scores = cross_val_score(model, train_post, train_target, cv=5, scoring='accuracy')\n",
    "    return -scores.mean()  # Minimize negative accuracy\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 300, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, -0.1)\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "best = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=50)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
