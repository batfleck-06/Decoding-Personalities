{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e401b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tqdm 4.64.0\n",
      "Uninstalling tqdm-4.64.0:\n",
      "  Would remove:\n",
      "    /opt/anaconda3/bin/tqdm\n",
      "    /opt/anaconda3/lib/python3.9/site-packages/tqdm-4.64.0.dist-info/*\n",
      "    /opt/anaconda3/lib/python3.9/site-packages/tqdm/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cea42d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tqdm' from partially initialized module 'tqdm' (most likely due to a circular import) (/opt/anaconda3/lib/python3.9/site-packages/tqdm/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#for hyperparameter tuning\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhyperopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin, tpe, hp\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/hyperopt/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trials_from_docs\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Domain\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin_pass_expr_memo_ctrl\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FMinIter\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coarse_utcnow\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m progress\n\u001b[1;32m     20\u001b[0m standard_library\u001b[38;5;241m.\u001b[39minstall_aliases()\n\u001b[1;32m     21\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/hyperopt/progress.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mProgress is reported using context managers.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mA progress context manager takes an `initial` and a `total` argument\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mand should yield an object with an `update(n)` method.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstd_out_err_redirect_tqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m std_out_err_redirect_tqdm\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtqdm_progress_callback\u001b[39m(initial, total):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tqdm/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageTk\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'tqdm' from partially initialized module 'tqdm' (most likely due to a circular import) (/opt/anaconda3/lib/python3.9/site-packages/tqdm/__init__.py)"
     ]
    }
   ],
   "source": [
    "# for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#For data Modeling\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "#For NLP\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "#Miscellaneous\n",
    "\n",
    "# for progress  bars\n",
    "\n",
    "\n",
    "#regular expressions\n",
    "import re\n",
    "\n",
    "#for .pkl file\n",
    "import joblib\n",
    "\n",
    "\n",
    "#for hyperparameter tuning\n",
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81af207b-69f1-436f-a0d6-95bfdee60906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/batfleck06/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383a8ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"mbti.csv\") \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96028f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the target sample size per class\n",
    "total_samples = df.shape[0]\n",
    "num_classes = len(df['type'].unique())\n",
    "target_samples_per_class = total_samples // num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e0f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_samples_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d26600e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396c53ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a191269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store posts for each personality type\n",
    "personality_posts = {ptype: [] for ptype in df['type'].unique()}\n",
    "\n",
    "# Iterate through each row and populate the dictionary\n",
    "for index, row in df.iterrows():\n",
    "    personality_posts[row['type']].append(row['posts'])\n",
    "\n",
    "# Lists to store balanced data\n",
    "balanced_features = []\n",
    "balanced_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17d9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each personality type\n",
    "for personality_type, posts in personality_posts.items():\n",
    "    num_samples = len(posts)\n",
    "    \n",
    "    if num_samples >= target_samples_per_class:\n",
    "        # Sample random indices\n",
    "        sampled_indices = np.random.choice(num_samples, target_samples_per_class, replace=False)\n",
    "        \n",
    "        # Add the sampled data to the balanced sets\n",
    "        balanced_features.extend([posts[i] for i in sampled_indices])\n",
    "        balanced_labels.extend([personality_type] * target_samples_per_class)\n",
    "    else:\n",
    "        # If fewer posts than target_samples_per_class, use all available posts\n",
    "        balanced_features.extend(posts)\n",
    "        balanced_labels.extend([personality_type] * num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed8717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "shuffled_indices = np.random.permutation(len(balanced_features))\n",
    "balanced_features = [balanced_features[i] for i in shuffled_indices]\n",
    "balanced_labels = [balanced_labels[i] for i in shuffled_indices]\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    balanced_features, balanced_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6920fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    data_length = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_text = []\n",
    "    for sentence in tqdm(data):\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        sentence = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', ' ', sentence)\n",
    "\n",
    "        # Remove non-alphanumeric characters\n",
    "        sentence = re.sub('[^0-9a-z]', ' ', sentence)\n",
    "\n",
    "        data_length.append(len(sentence.split()))\n",
    "        cleaned_text.append(sentence)\n",
    "    return cleaned_text, data_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ce205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3896/3896 [00:02<00:00, 1432.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clean train and test features\n",
    "cleaned_train_features, train_data_lengths = clean_text(train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68710bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 974/974 [00:00<00:00, 1442.96it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_test_features, test_data_lengths = clean_text(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d44ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word) > 2]\n",
    "\n",
    "# Initialize the Lemmatizer\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "256940f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize cleaned train and test features\n",
    "lemmatized_train_features = [lemmatizer(sentence) for sentence in cleaned_train_features]\n",
    "lemmatized_test_features = [lemmatizer(sentence) for sentence in cleaned_test_features]\n",
    "\n",
    "# Convert lemmatized features back to sentences\n",
    "lemmatized_train_sentences = [' '.join(sentence) for sentence in lemmatized_train_features]\n",
    "lemmatized_test_sentences = [' '.join(sentence) for sentence in lemmatized_test_features]\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64c3cb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on lemmatized training data and transform training and testing data\n",
    "train_post = vectorizer.fit_transform(lemmatized_train_sentences)\n",
    "test_post = vectorizer.transform(lemmatized_test_sentences)\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cffc9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels for both training and testing sets\n",
    "train_target = label_encoder.fit_transform(train_labels)\n",
    "test_target = label_encoder.transform(test_labels)\n",
    "# Save the label encoder for later use\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a659b54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENFJ': 0,\n",
       " 'ENFP': 1,\n",
       " 'ENTJ': 2,\n",
       " 'ENTP': 3,\n",
       " 'ESFJ': 4,\n",
       " 'ESFP': 5,\n",
       " 'ESTJ': 6,\n",
       " 'ESTP': 7,\n",
       " 'INFJ': 8,\n",
       " 'INFP': 9,\n",
       " 'INTJ': 10,\n",
       " 'INTP': 11,\n",
       " 'ISFJ': 12,\n",
       " 'ISFP': 13,\n",
       " 'ISTJ': 14,\n",
       " 'ISTP': 15}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mapping between encoded labels and original labels\n",
    "encoded_to_original_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "encoded_to_original_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d0500bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_accuracy={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "140d1f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb=XGBClassifier(max_depth=5, n_estimators=50, learning_rate=0.1)\n",
    "model_xgb.fit(train_post,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccf48dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train classification report \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       1.00      0.99      1.00       145\n",
      "        ENFP       0.98      0.98      0.98       442\n",
      "        ENTJ       1.00      1.00      1.00       190\n",
      "        ENTP       1.00      0.98      0.99       433\n",
      "        ESFJ       1.00      0.94      0.97        32\n",
      "        ESFP       1.00      1.00      1.00        39\n",
      "        ESTJ       1.00      1.00      1.00        31\n",
      "        ESTP       1.00      1.00      1.00        74\n",
      "        INFJ       0.98      0.99      0.98       419\n",
      "        INFP       0.97      0.99      0.98       442\n",
      "        INTJ       0.98      0.99      0.99       446\n",
      "        INTP       0.99      0.99      0.99       426\n",
      "        ISFJ       0.99      0.99      0.99       138\n",
      "        ISFP       1.00      0.99      1.00       210\n",
      "        ISTJ       1.00      1.00      1.00       156\n",
      "        ISTP       0.99      0.99      0.99       273\n",
      "\n",
      "    accuracy                           0.99      3896\n",
      "   macro avg       0.99      0.99      0.99      3896\n",
      "weighted avg       0.99      0.99      0.99      3896\n",
      "\n",
      "test classification report \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.71      0.56      0.63        45\n",
      "        ENFP       0.66      0.71      0.68       100\n",
      "        ENTJ       0.79      0.63      0.70        41\n",
      "        ENTP       0.67      0.75      0.71       109\n",
      "        ESFJ       1.00      0.20      0.33        10\n",
      "        ESFP       0.00      0.00      0.00         9\n",
      "        ESTJ       0.80      0.50      0.62         8\n",
      "        ESTP       0.43      0.20      0.27        15\n",
      "        INFJ       0.72      0.65      0.68       123\n",
      "        INFP       0.61      0.71      0.65       100\n",
      "        INTJ       0.58      0.73      0.65        96\n",
      "        INTP       0.71      0.68      0.69       116\n",
      "        ISFJ       0.72      0.64      0.68        28\n",
      "        ISFP       0.72      0.72      0.72        61\n",
      "        ISTJ       0.81      0.61      0.70        49\n",
      "        ISTP       0.68      0.83      0.75        64\n",
      "\n",
      "    accuracy                           0.68       974\n",
      "   macro avg       0.66      0.57      0.59       974\n",
      "weighted avg       0.68      0.68      0.67       974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('train classification report \\n ',classification_report(train_target,model_xgb.predict(train_post),target_names=label_encoder.inverse_transform([i for i in range(16)])))\n",
    "print('test classification report \\n ',classification_report(test_target,model_xgb.predict(test_post),target_names=label_encoder.inverse_transform([i for i in range(16)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88ca3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_accuracy['XGBoost Classifier']=accuracy_score(test_target,model_xgb.predict(test_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "934117fb-96d4-45cc-b81b-0115527f006c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_xgb, \"xgb_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80867fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a721e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logistic_reg_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log=LogisticRegression(max_iter=3000,C=0.5,n_jobs=-1)\n",
    "model_log.fit(train_post,train_target)\n",
    "joblib.dump(model_log, \"logistic_reg_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a4053c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train classification report \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.85      0.35      0.50       145\n",
      "        ENFP       0.73      0.85      0.79       442\n",
      "        ENTJ       0.88      0.50      0.64       190\n",
      "        ENTP       0.72      0.87      0.79       433\n",
      "        ESFJ       0.00      0.00      0.00        32\n",
      "        ESFP       0.00      0.00      0.00        39\n",
      "        ESTJ       0.00      0.00      0.00        31\n",
      "        ESTP       1.00      0.12      0.22        74\n",
      "        INFJ       0.72      0.83      0.77       419\n",
      "        INFP       0.67      0.86      0.76       442\n",
      "        INTJ       0.71      0.87      0.78       446\n",
      "        INTP       0.71      0.89      0.79       426\n",
      "        ISFJ       0.88      0.44      0.59       138\n",
      "        ISFP       0.88      0.58      0.70       210\n",
      "        ISTJ       0.87      0.47      0.61       156\n",
      "        ISTP       0.85      0.81      0.83       273\n",
      "\n",
      "    accuracy                           0.74      3896\n",
      "   macro avg       0.65      0.53      0.55      3896\n",
      "weighted avg       0.74      0.74      0.72      3896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('train classification report \\n ',classification_report(train_target,model_log.predict(train_post),target_names=label_encoder.inverse_transform([i for i in range(16)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5656270",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_accuracy['logistic regression']=accuracy_score(test_target,model_log.predict(test_post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78d76554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.675565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.616016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models  Test accuracy\n",
       "0   XGBoost Classifier       0.675565\n",
       "1  logistic regression       0.616016"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy=pd.DataFrame(models_accuracy.items(),columns=['Models','Test accuracy'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae42d7",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebcf6d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.66798203\n",
      "Iteration 2, loss = 2.41894378\n",
      "Iteration 3, loss = 2.19807920\n",
      "Iteration 4, loss = 1.87780397\n",
      "Iteration 5, loss = 1.50938358\n",
      "Iteration 6, loss = 1.14694537\n",
      "Iteration 7, loss = 0.82868617\n",
      "Iteration 8, loss = 0.58325271\n",
      "Iteration 9, loss = 0.39832961\n",
      "Iteration 10, loss = 0.27044612\n",
      "Iteration 11, loss = 0.18658638\n",
      "Iteration 12, loss = 0.13114428\n",
      "Iteration 13, loss = 0.09251532\n",
      "Iteration 14, loss = 0.06755116\n",
      "Iteration 15, loss = 0.05086311\n",
      "Iteration 16, loss = 0.03898696\n",
      "Iteration 17, loss = 0.03075298\n",
      "Iteration 18, loss = 0.02502991\n",
      "Iteration 19, loss = 0.02083341\n",
      "Iteration 20, loss = 0.01762096\n",
      "Iteration 21, loss = 0.01512534\n",
      "Iteration 22, loss = 0.01325279\n",
      "Iteration 23, loss = 0.01170937\n",
      "Iteration 24, loss = 0.01043851\n",
      "Iteration 25, loss = 0.00942500\n",
      "Iteration 26, loss = 0.00856818\n",
      "Iteration 27, loss = 0.00782015\n",
      "Iteration 28, loss = 0.00718733\n",
      "Iteration 29, loss = 0.00664802\n",
      "Iteration 30, loss = 0.00618330\n",
      "Iteration 31, loss = 0.00577977\n",
      "Iteration 32, loss = 0.00540889\n",
      "Iteration 33, loss = 0.00509548\n",
      "Iteration 34, loss = 0.00480772\n",
      "Iteration 35, loss = 0.00455723\n",
      "Iteration 36, loss = 0.00433103\n",
      "Iteration 37, loss = 0.00412010\n",
      "Iteration 38, loss = 0.00393185\n",
      "Iteration 39, loss = 0.00376567\n",
      "Iteration 40, loss = 0.00360951\n",
      "Iteration 41, loss = 0.00347215\n",
      "Iteration 42, loss = 0.00334392\n",
      "Iteration 43, loss = 0.00322671\n",
      "Iteration 44, loss = 0.00311779\n",
      "Iteration 45, loss = 0.00301870\n",
      "Iteration 46, loss = 0.00292306\n",
      "Iteration 47, loss = 0.00283732\n",
      "Iteration 48, loss = 0.00275844\n",
      "Iteration 49, loss = 0.00268495\n",
      "Iteration 50, loss = 0.00261522\n",
      "Iteration 51, loss = 0.00255047\n",
      "Iteration 52, loss = 0.00249091\n",
      "Iteration 53, loss = 0.00243377\n",
      "Iteration 54, loss = 0.00238106\n",
      "Iteration 55, loss = 0.00233143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=1000, random_state=42,\n",
       "              verbose=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an MLP classifier\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=1000, alpha=1e-4,\n",
    "                         solver='adam', verbose=10, random_state=42, learning_rate_init=0.001)\n",
    "\n",
    "# Fit the MLP classifier on the training data\n",
    "model_mlp.fit(train_post, train_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef2cdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       1.00      1.00      1.00       147\n",
      "        ENFP       1.00      1.00      1.00       441\n",
      "        ENTJ       1.00      1.00      1.00       185\n",
      "        ENTP       1.00      1.00      1.00       443\n",
      "        ESFJ       1.00      1.00      1.00        33\n",
      "        ESFP       1.00      1.00      1.00        36\n",
      "        ESTJ       1.00      1.00      1.00        30\n",
      "        ESTP       1.00      1.00      1.00        72\n",
      "        INFJ       1.00      1.00      1.00       425\n",
      "        INFP       1.00      1.00      1.00       439\n",
      "        INTJ       1.00      1.00      1.00       443\n",
      "        INTP       1.00      1.00      1.00       436\n",
      "        ISFJ       1.00      1.00      1.00       133\n",
      "        ISFP       1.00      1.00      1.00       217\n",
      "        ISTJ       1.00      1.00      1.00       159\n",
      "        ISTP       1.00      1.00      1.00       257\n",
      "\n",
      "    accuracy                           1.00      3896\n",
      "   macro avg       1.00      1.00      1.00      3896\n",
      "weighted avg       1.00      1.00      1.00      3896\n",
      "\n",
      "Test classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.48      0.26      0.33        43\n",
      "        ENFP       0.47      0.55      0.51       101\n",
      "        ENTJ       0.58      0.39      0.47        46\n",
      "        ENTP       0.48      0.60      0.53        99\n",
      "        ESFJ       0.50      0.33      0.40         9\n",
      "        ESFP       0.00      0.00      0.00        12\n",
      "        ESTJ       0.00      0.00      0.00         9\n",
      "        ESTP       0.62      0.29      0.40        17\n",
      "        INFJ       0.50      0.50      0.50       117\n",
      "        INFP       0.47      0.51      0.49       103\n",
      "        INTJ       0.50      0.51      0.50        99\n",
      "        INTP       0.50      0.61      0.55       106\n",
      "        ISFJ       0.62      0.55      0.58        33\n",
      "        ISFP       0.56      0.54      0.55        54\n",
      "        ISTJ       0.54      0.46      0.49        46\n",
      "        ISTP       0.69      0.69      0.69        80\n",
      "\n",
      "    accuracy                           0.51       974\n",
      "   macro avg       0.47      0.42      0.44       974\n",
      "weighted avg       0.51      0.51      0.51       974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Generate classification reports for the training and test data\n",
    "print('Train classification report:\\n', classification_report(train_target, model_mlp.predict(train_post), target_names=label_encoder.inverse_transform([i for i in range(16)])))\n",
    "print('Test classification report:\\n', classification_report(test_target, model_mlp.predict(test_post), target_names=label_encoder.inverse_transform([i for i in range(16)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4d3da36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you already have the MLP model named model_mlp\n",
    "mlp_accuracy = accuracy_score(test_target, model_mlp.predict(test_post))\n",
    "\n",
    "# Store the accuracy score in the models_accuracy dictionary\n",
    "models_accuracy['MLP'] = mlp_accuracy\n",
    "\n",
    "# Save the trained MLP model to a file using joblib\n",
    "joblib.dump(model_mlp, \"mlp_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0daf869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.514374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Models  Test accuracy\n",
       "0    MLP       0.514374"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy=pd.DataFrame(models_accuracy.items(),columns=['Models','Test accuracy'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaad774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8774.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted personality type: INTJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "ml_model = joblib.load(\"ml_model.pkl\")\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Preprocess the random text\n",
    "random_text = \"That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached with BS\"\n",
    "\n",
    "\n",
    "# Clean the random text\n",
    "cleaned_random_text, _ = clean_text([random_text])\n",
    "\n",
    "# Lemmatize the cleaned random text\n",
    "lemmatized_random_text = lemmatizer(cleaned_random_text[0])\n",
    "\n",
    "# Convert lemmatized random text back to a sentence\n",
    "lemmatized_random_text_sentence = ' '.join(lemmatized_random_text)\n",
    "\n",
    "# Transform the lemmatized random text using the TF-IDF vectorizer\n",
    "text_features = tfidf_vectorizer.transform([lemmatized_random_text_sentence])\n",
    "\n",
    "# Make predictions\n",
    "ml_prediction = ml_model.predict(text_features)\n",
    "ml_prediction_label = label_encoder.inverse_transform(ml_prediction)[0]\n",
    "\n",
    "print(\"Predicted personality type:\", ml_prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_data_tensor = torch.Tensor(train_post.toarray())\n",
    "train_target_tensor = torch.LongTensor(train_target)\n",
    "test_data_tensor = torch.Tensor(test_post.toarray())\n",
    "test_target_tensor = torch.LongTensor(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adb9ebcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m targets \u001b[38;5;241m=\u001b[39m train_target_tensor[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m rnn_model(inputs)\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [33], line 11\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x)\n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Assuming batch_first=True\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Define a simple RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Assuming batch_first=True\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = train_data_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(label_encoder.classes_)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize the RNN model\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_data_tensor), batch_size):\n",
    "        inputs = train_data_tensor[i:i+batch_size]\n",
    "        targets = train_target_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained RNN model\n",
    "torch.save(rnn_model.state_dict(), \"trained_rnn_model.pth\")\n",
    "\n",
    "# Load the trained RNN model\n",
    "loaded_rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "loaded_rnn_model.load_state_dict(torch.load(\"trained_rnn_model.pth\"))\n",
    "\n",
    "# Make predictions using the trained RNN model\n",
    "with torch.no_grad():\n",
    "    test_outputs = loaded_rnn_model(test_data_tensor)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted.numpy())\n",
    "\n",
    "print(\"Predicted personality types:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60b3f9-535d-4d73-a22a-272d209d1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = XGBClassifier(**params)\n",
    "    # Change the scoring metric as needed\n",
    "    scores = cross_val_score(model, train_post, train_target, cv=5, scoring='accuracy')\n",
    "    return -scores.mean()  # Minimize negative accuracy\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 300, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, -0.1)\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "best = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=50)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "486cf4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.9/site-packages (10.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a0637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
